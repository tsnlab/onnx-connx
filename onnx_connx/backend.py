import argparse
import os
import random
import tempfile
import time
import statistics
from typing import Any, Dict, List, Optional, Sequence, Text, Tuple

import numpy
import onnx.checker
from onnx import ModelProto, NodeProto, numpy_helper

from . import get_DataType
from .backend_rep import BackendRep
from .compiler import compile_from_model
from .opset import get_attrset


class Backend(object):
    @classmethod
    def is_compatible(cls,
                      model,  # type: ModelProto
                      device='CPU',  # type: Text
                      **kwargs  # type: Any
                      ):  # type: (...) -> bool

        specs = []
        for i in range(len(model.opset_import)):
            opset_import = model.opset_import[i]
            specs.append({'domain': opset_import.domain, 'version': opset_import.version})

        attrset, attrver = get_attrset(specs)

        for i in range(len(model.graph.node)):
            if model.graph.node[i].op_type not in attrset or attrset[model.graph.node[i].op_type] is None:
                # print('Not supported op_type:', model.graph.node[i].op_type)
                return False

        return True

    @classmethod
    def prepare(cls,
                model: ModelProto,
                device: str = 'CPU',
                **kwargs
                ) -> BackendRep:
        onnx.checker.check_model(model)

        if len(model.opset_import) == 0:
            opset_id = onnx.OperatorSetIdProto()
            opset_id.domain = ''
            opset_id.version = 1
            model.opset_import.append(opset_id)

        if 'out' in kwargs:
            model_path = kwargs['out']
            os.makedirs(model_path, exist_ok=True)

            compile_from_model(model, model_path)
            return BackendRep(model_path, **kwargs)
        else:
            model_path = os.path.join(tempfile.gettempdir(), f'connx.{time.time() + random.random()}')
            compile_from_model(model, model_path)
            return BackendRep(model_path, delete_path=True, **kwargs)

    @classmethod
    def run_model(cls,
                  model,  # type: ModelProto
                  inputs,  # type: Any
                  device='CPU',  # type: Text
                  **kwargs  # type: Any
                  ):  # type: (...) -> Tuple[Any, ...]
        backend = cls.prepare(model, device, **kwargs)
        return backend.run(inputs)

    @classmethod
    def run_node(cls,
                 node,  # type: NodeProto
                 inputs,  # type: Any
                 device='CPU',  # type: Text
                 outputs_info=None,  # type: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]]
                 **kwargs  # type: Dict[Text, Any]
                 ):  # type: (...) -> Optional[Tuple[Any, ...]]
        model = onnx.ModelProto()
        model.ir_version = onnx.IR_VERSION

        opset_id = onnx.OperatorSetIdProto()
        opset_id.domain = ''
        opset_id.version = onnx.defs.onnx_opset_version()
        model.opset_import.append(opset_id)

        model.graph.name = node.name + ' test (auto generated by connx backend)'
        model.graph.node.append(node)

        for input_, name in zip(inputs, node.input):
            value_info = onnx.helper.make_tensor_value_info(name, get_DataType(input_.dtype), input_.shape)
            model.graph.input.append(value_info)

        for name in node.output:
            value_info = onnx.helper.make_tensor_value_info(name, onnx.TensorProto.DataType.FLOAT, [0])
            model.graph.output.append(value_info)

        return cls.run_model(model, inputs, device, *kwargs)

    @classmethod
    def supports_device(cls, device):  # type: (Text) -> bool
        return device in ['CPU', 'cpu']


def main(args):
    onnx_path = args.onnx[0]
    input_paths = args.pb

    model = onnx.load_model(onnx_path)
    inputs = []

    for input_path in input_paths:
        with open(input_path, 'rb') as f:
            tensor = onnx.TensorProto()
            tensor.ParseFromString(f.read())

            inputs.append(numpy_helper.to_array(tensor))

    kwargs = {}

    if args.output is not None:
        kwargs['out'] = args.output

    if args.performance is not None:
        kwargs['loop_count'] = args.performance

    backend = Backend.prepare(model, **kwargs)
    outputs = backend.run(inputs)

    if args.performance:
        outputs: List[float]
        minimum = min(outputs)
        maximum = max(outputs)
        mean = statistics.mean(outputs)
        total = sum(outputs)

        print(f'minimum: {minimum * (10 ** 9):12,.0f}')
        print(f'mean:    {mean * (10 ** 9):12,.0f}')
        print(f'maximum: {maximum * (10 ** 9):12,.0f}')
        print(f'total:   {total * (10 ** 9):12,.0f}')

    elif isinstance(outputs, tuple):
        for output in outputs:
            print(output)
    else:
        print(outputs)


def run():
    parser = argparse.ArgumentParser(description='CONNX Backend')
    parser.add_argument('onnx', metavar='onnx', nargs=1, help='an input ONNX model file')
    parser.add_argument('pb', metavar='pb', nargs='*', help='tensor pb files')
    parser.add_argument('-o', '--output', metavar='output directory', type=str, nargs='?',
                        help='connx output directory(default is temporary directory)')
    parser.add_argument('-p', '--performance', metavar='loop count', nargs='?', type=int,
                        help='Pass performance test option to connx')

    args = parser.parse_args()

    main(args)


if __name__ == '__main__':
    run()
