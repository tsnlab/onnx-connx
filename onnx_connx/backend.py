import argparse
import os
import random
import tempfile
import time
from typing import Any, Dict, Optional, Sequence, Text, Tuple

import numpy
import onnx.checker
from onnx import ModelProto, NodeProto, numpy_helper

from . import get_DataType
from .backend_rep import BackendRep
from .compiler import compile_from_model
from .opset import get_attrset


class Backend(object):
    @classmethod
    def is_compatible(cls,
                      model,  # type: ModelProto
                      device='CPU',  # type: Text
                      **kwargs  # type: Any
                      ):  # type: (...) -> bool

        specs = []
        for i in range(len(model.opset_import)):
            opset_import = model.opset_import[i]
            specs.append({'domain': opset_import.domain, 'version': opset_import.version})

        attrset, attrver = get_attrset(specs)

        for i in range(len(model.graph.node)):
            if model.graph.node[i].op_type not in attrset or attrset[model.graph.node[i].op_type] is None:
                # print('Not supported op_type:', model.graph.node[i].op_type)
                return False

        return True

    @classmethod
    def prepare(cls,
                model: ModelProto,
                device: str = 'CPU',
                **kwargs
                ) -> BackendRep:
        onnx.checker.check_model(model)

        if len(model.opset_import) == 0:
            opset_id = onnx.OperatorSetIdProto()
            opset_id.domain = ''
            opset_id.version = 1
            model.opset_import.append(opset_id)

        if 'out' in kwargs:
            model_path = kwargs['out']
            os.makedirs(model_path, exist_ok=True)

            compile_from_model(model, model_path)
            return BackendRep(model_path, **kwargs)
        else:
            model_path = os.path.join(tempfile.gettempdir(), f'connx.{time.time() + random.random()}')
            compile_from_model(model, model_path)
            return BackendRep(model_path, delete_path=True, **kwargs)

    @classmethod
    def run_model(cls,
                  model,  # type: ModelProto
                  inputs,  # type: Any
                  device='CPU',  # type: Text
                  **kwargs  # type: Any
                  ):  # type: (...) -> Tuple[Any, ...]
        backend = cls.prepare(model, device, **kwargs)
        return backend.run(inputs)

    @classmethod
    def run_node(cls,
                 node,  # type: NodeProto
                 inputs,  # type: Any
                 device='CPU',  # type: Text
                 outputs_info=None,  # type: Optional[Sequence[Tuple[numpy.dtype, Tuple[int, ...]]]]
                 **kwargs  # type: Dict[Text, Any]
                 ):  # type: (...) -> Optional[Tuple[Any, ...]]
        model = onnx.ModelProto()
        model.ir_version = onnx.IR_VERSION

        opset_id = onnx.OperatorSetIdProto()
        opset_id.domain = ''
        opset_id.version = onnx.defs.onnx_opset_version()
        model.opset_import.append(opset_id)

        model.graph.name = node.name + ' test (auto generated by connx backend)'
        model.graph.node.append(node)

        for input, name in zip(inputs, node.input):
            value_info = onnx.helper.make_tensor_value_info(name, get_DataType(input.dtype), input.shape)
            model.graph.input.append(value_info)

        for name in node.output:
            value_info = onnx.helper.make_tensor_value_info(name, onnx.TensorProto.DataType.FLOAT, [0])
            model.graph.output.append(value_info)

        return cls.run_model(model, inputs, device, *kwargs)

    @classmethod
    def supports_device(cls, device):  # type: (Text) -> bool
        return device in ['CPU', 'cpu']


def main(args):
    onnx_path = args.onnx[0]
    input_paths = args.pb

    model = onnx.load_model(onnx_path)
    inputs = []

    for input_path in input_paths:
        with open(input_path, 'rb') as f:
            tensor = onnx.TensorProto()
            tensor.ParseFromString(f.read())

            inputs.append(numpy_helper.to_array(tensor))

    kwargs = {}

    if args.output is not None:
        kwargs['out'] = args.output

    if args.performance is not None:
        kwargs['loop_count'] = args.performance

    backend = Backend.prepare(model, **kwargs)
    outputs = backend.run(inputs)

    if isinstance(outputs, tuple):
        for output in outputs:
            print(output)
    else:
        print(outputs)


def run():
    parser = argparse.ArgumentParser(description='CONNX Backend')
    parser.add_argument('onnx', metavar='onnx', nargs=1, help='an input ONNX model file')
    parser.add_argument('pb', metavar='pb', nargs='*', help='tensor pb files')
    parser.add_argument('-o', '--output', metavar='output directory', type=str, nargs='?',
                        help='connx output directory(default is temporary directory)')
    parser.add_argument('-p', '--performance', metavar='loop count', nargs='?', type=int,
                        help='Pass performance test option to connx')

    args = parser.parse_args()

    main(args)


if __name__ == '__main__':
    run()
